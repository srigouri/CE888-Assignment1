# -*- coding: utf-8 -*-
"""Jobs_Causal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t-wzKFJK4jmu4kziXcjTsjLCpWImuqft

# Causal Inference on JOBS Dataset:

---
Description:

 -Using Linear Regression,Logistic Regression with Inverse Propensity Weighting (IPW),Random Forest Regression,Random forest with Inverse Propensity Weighting (IPW), CATE estimator X-Learner with RandomForest as base learner to estimate causal effects in JOBS data.
 (Reference: CE888 Lab4 Task).

 -Hyperparameter optimization using GridSearchCV.

 -Evaluation metrics are called from causalfuncs.py
(Reference: https://github.com/dmachlanski/CE888_2022/blob/main/project/metrics.py)

 -Data source: https://github.com/dmachlanski/CE888_2022/tree/main/project/data


---

#### Packages
Loading Required Packages:
"""

!pip install econml

"""
Importing Libraries:"""

from econml.metalearners import XLearner
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import scipy.stats as st
import matplotlib.pyplot as plt
import seaborn as sns
from causalfuncs import *

"""#### Data

Loading Jobs data from https://raw.githubusercontent.com/dmachlanski/CE888_2022/main/project/data/jobs.csv: using pandas.
"""

jobs_data=pd.read_csv('https://raw.githubusercontent.com/dmachlanski/CE888_2022/main/project/data/jobs.csv')

"""Exploring JOBS data:"""

jobs_data.head()

"""To check number of rows and columns in JOBS dataset:

"""

jobs_data.shape

"""JOBS data has got 20 columns in which columns with names x1 to x17 are input features, t denotes for treatment, y is the outcome, and e denotes Effect.The values for particular columns are extracted using integer-location based indexing and
assigning these values to the variables X,T,Yand e respectively.
"""

X=jobs_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17']]
T=jobs_data.iloc[:,17:18].values
Y=jobs_data.iloc[:,18:19].values
e=jobs_data.iloc[:,19:20].values

"""####Data Pre-processing:

Splitting the Jobs data to Training, validation and testing data.First splitting complete dataset to Training and Testing data in 80/20 ratio.Then again splitting training data to Training and Validation data in 80/20 ratio.
"""

X_train, X_test, T_train, T_test, Y_train, Y_test,e_train,e_test = train_test_split(X, T, Y,e, test_size=0.2)
X_train, X_val,T_train,T_val,Y_train,Y_val,e_train,e_val = train_test_split(X_train,T_train,Y_train,e_train,test_size=0.20)

"""Checking the shapes of training,Validation,Testing Data

"""

print("X_train shape: {}".format(X_train.shape))
print("X_val shape: {}".format(X_val.shape))
print("X_test shape: {}".format(X_test.shape))
print("T_train shape: {}".format(T_train.shape))
print("T_val shape: {}".format(T_val.shape))
print("T_test shape: {}".format(T_test.shape))
print("Y_train shape: {}".format(Y_train.shape))
print("Y_val shape: {}".format(Y_val.shape))
print("Y_test shape: {}".format(Y_test.shape))
print("e_train shape: {}".format(e_train.shape))
print("e_val shape: {}".format(e_val.shape))
print("e_test shape: {}".format(e_test.shape))

"""Plotting histograms of outcome variable and treatment variable:"""

plt.hist(Y)

plt.hist(T)

plt.hist(e)

"""The above histograms of Y shows that the data is imbalanced. both y and t are binary with 0 and 1 as values.

Standardizing the input training data X. No standardization required for treatment variable T as the data is binary.
"""

scaler_x = StandardScaler()
X_train = scaler_x.fit_transform(X_train)
X_val = scaler_x.transform(X_val)
X_test = scaler_x.transform(X_test)

"""computing the means of effect in training,validation,test data"""

np.mean(e_train),np.mean(e_val),np.mean(e_test)

"""Checking the shape of outcome variable:"""

Y_train.shape

"""#### 3.Training the models & Making predictions to compute relevant metrics:
#### 4.Training classifier to predict propensity scores and using same regressors 

####Linear Regression

Training The Model: Using four estimators for training the model:


Concatenating X variable with 25 input features and T i.e treatment 
variable to train the model along with treatment.

Fitting Linear Regressor on training data:
"""

lr = LinearRegression()
XT_train = np.concatenate((X_train,T_train),axis=1)

lr.fit(XT_train, Y_train.ravel())

#For validation of model, y1_val and y0_val are predicted by setting treatment to 1 and 0 respectively.
#Treatment variable (T=0,T=1) is again merged accordingly with X data.
#Predicted outcomes for both treated and controlled for each individuals as y1_val,y0_val and obtained treatement effect using e= y1-y0.
xt0_val = np.concatenate([X_val, np.zeros_like(T_val)], axis=1)
lr_y0_val = lr.predict(xt0_val)

xt1_val = np.concatenate([X_val, np.ones_like(T_val)], axis=1)
lr_y1_val = lr.predict(xt1_val)

lr_t_val = lr_y1_val - lr_y0_val

#Predictions using Test data:
#y1_test and y0_test are predicted by setting treatment to 1 and 0 respectively.
#Setting T to a 1 and 0 for all individuals using zeros_like and one_like and merge with X_test to obtain effect estimates.
#Outcomes for both treated and controlled for each individuals as y1_test,y0_test and obtained effect using e=y1-y0.

xt0_test = np.concatenate([X_test, np.zeros_like(T_test)], axis=1)
lr_y0_test = lr.predict(xt0_test)

xt1_test = np.concatenate([X_test, np.ones_like(T_test)], axis=1)
lr_y1_test = lr.predict(xt1_test)

lr_e_test = lr_y1_test - lr_y0_test

####Logistic Regression Classifier with Inverse Propensity Score(IPW):

lr_clf = LogisticRegression()
weights = get_ps_weights(lr_clf, X_train, T_train)

lr_ipsw = LinearRegression()

lr_ipsw.fit(XT_train, Y_train.flatten(), sample_weight=weights)

lr_ipsw_y0 = lr_ipsw.predict(xt0_val)
lr_ipsw_y1 = lr_ipsw.predict(xt1_val)

lr_ipsw_e = lr_ipsw_y1 - lr_ipsw_y0

lr_ipsw_y0_test = lr_ipsw.predict(xt0_test)
lr_ipsw_y1_test = lr_ipsw.predict(xt1_test)

lr_ipsw_e_test = lr_ipsw_y1_test - lr_ipsw_y0_test

"""#### Random Forest Regression

"""

#Fitting Random Forest Regressor on training data:

rf = RandomForestRegressor()
rf.fit(XT_train, Y_train.flatten())

#Using Random Forest Regression model,validating outcomes for both treated and controlled 
#for each individuals as y1_val,y0_val and obtained treatement effect using e= y1-y0.

rf_y0_val = rf.predict(xt0_val)
rf_y1_val = rf.predict(xt1_val)

rf_e_val = rf_y1_val - rf_y0_val


rf_y0_test = rf.predict(xt0_test)
rf_y1_test = rf.predict(xt1_test)

rf_e_test = rf_y1_test - rf_y0_test

"""####RandomForestClassifier with Inverse Propensity Score(IPW):

Training using RandomForestClassifier extending with with the Inverse Propensity Weighting (IPW),to model unit's probability of receiving the treatment, P(ti|xi). This is a classic binary classification problem using input X,treatment T. P(ti|xi)  is called a propensity score.

To get the sample weights, get_ps_weights function is called from causalfuncs.py which is available at https://github.com/srigouri/CE888-Causal/blob/main/causalfuncs.py
"""

from causalfuncs import get_ps_weights
prop_clf = RandomForestClassifier()
weights = get_ps_weights(prop_clf, X_train, T_train)

rf_ipsw = RandomForestRegressor()
rf_ipsw.fit(XT_train, Y_train.flatten(), sample_weight=weights)

rf_ipsw_y0_test =rf_ipsw.predict(xt0_test) 
rf_ipsw_y1_test =rf_ipsw.predict(xt1_test) 

rf_ipsw_e_test = rf_ipsw_y1_test - rf_ipsw_y0_test

"""#### Hyper-parameter Tuning:

Using GridSearch and RandomForest Classifier.
"""

prop_clf.get_params().keys()

"""#### 5.X-Learner:
  CATE estimator X-learner is a meta-learner implemented via EconML. Used linear regressors and logistic classifiers to model and predict effect. We need not merge inputs and treatment to train the model. 
"""

xl = XLearner(models=LinearRegression(), propensity_model=LogisticRegression())
xl.fit(Y_train, T_train.flatten(), X=X_train)

xl_e_test = xl.effect(X_test)

"""##Evaluation

Metrics Chosen: ÏµATT  and  Policy_risk 


"""

# Using 'abs_att' function from causalfuncs.py, Average Treatment Effect on the Treated.
lr_att_test = abs_att(lr_e_test.reshape(-1,1),Y_test,T_test,e_test)
lr_ipsw_att_test = abs_att(lr_ipsw_e_test.reshape(-1,1),Y_test,T_test,e_test)
rf_att_test = abs_att(rf_e_test.reshape(-1,1),Y_test,T_test,e_test)
rf_ipsw_att_test = abs_att(rf_ipsw_e_test.reshape(-1,1),Y_test,T_test,e_test)
xl_att_test = abs_att(xl_e_test.reshape(-1,1),Y_test,T_test,e_test) 

# Using 'policy_risk' function from causalfuncs.py, risk of the policy defined by predicted effect.
lr_pr_test =policy_risk(lr_e_test.reshape(-1,1),Y_test,T_test,e_test) 
lr_ipsw_pr_test =policy_risk(lr_ipsw_e_test.reshape(-1,1),Y_test,T_test,e_test) 
rf_pr_test =policy_risk(rf_e_test.reshape(-1,1),Y_test,T_test,e_test) 
rf_ipsw_pr_test = policy_risk(rf_ipsw_e_test.reshape(-1,1),Y_test,T_test,e_test) 
xl_pr_test = policy_risk(xl_e_test.reshape(-1,1),Y_test,T_test,e_test)

results = []
results.append(['LR', lr_att_test, lr_pr_test])
results.append(['LR (IPSW)', lr_ipsw_att_test, lr_ipsw_pr_test])
results.append(['RF', rf_att_test, rf_pr_test])
results.append(['RF (IPSW)', rf_ipsw_att_test, rf_ipsw_pr_test])
results.append(['XL', xl_att_test, xl_pr_test])

cols = ['Method', 'ATT test', 'Policy_risk test']

df_jobs = pd.DataFrame(results, columns=cols)
df_jobs

#### Confidence Intervals

"""Confidence intervals of predicted ATEs"""

lr_ate_bounds = mean_ci(lr_e_test)
lr_ipsw_ate_bounds = mean_ci(lr_ipsw_e_test)
rf_ate_bounds = mean_ci(rf_e_test)
rf_ipsw_ate_bounds = mean_ci(rf_ipsw_e_test)
xl_ate_bounds = mean_ci(xl_e_test)


results = []
results.append(['LR', lr_ate_bounds[0], lr_ate_bounds[1], lr_ate_bounds[2]])
results.append(['LR (IPSW)', lr_ipsw_ate_bounds[0], lr_ipsw_ate_bounds[1], lr_ipsw_ate_bounds[2]])
results.append(['RF', rf_ate_bounds[0], rf_ate_bounds[1], rf_ate_bounds[2]])
results.append(['RF (IPSW)', rf_ipsw_ate_bounds[0], rf_ipsw_ate_bounds[1], rf_ipsw_ate_bounds[2]])
results.append(['XL', xl_ate_bounds[0], xl_ate_bounds[1], xl_ate_bounds[2]])

cols = ['Method', 'ATE mean', 'CI lower', 'CI upper']

df2 = pd.DataFrame(results, columns=cols)
df2

"""#### Visualizations

Box plot showing treatment effect with different models:
"""

plt.figure()
plt.boxplot([lr_e_test,lr_ipsw_e_test,rf_e_test, rf_ipsw_e_test, xl_e_test.flatten()], labels=['LR','LR(IPSW)','RF', 'RF (IPSW)', 'X-learner'])
plt.ylabel('Treatment Effect')

plt.show()

"""Scatter plot showing treatment effect with different models:"""

plt.figure(figsize=(12, 10))
m_size = 10
plt.scatter(Y_test,lr_e_test, label="LR", s=m_size)
plt.scatter(Y_test,lr_ipsw_e_test, label="LR (IPSW)", s=m_size)
plt.scatter(Y_test,rf_e_test, label="RF", s=m_size)
plt.scatter(Y_test,rf_ipsw_e_test, label="RF (IPSW)", s=m_size)
plt.scatter(Y_test,xl_e_test, label="X-learner", s=m_size)
plt.xlabel('X')
plt.ylabel('Treatment Effect')
plt.legend()
plt.show()